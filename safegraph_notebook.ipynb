{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# set cities of interest\n",
    "cities = [('Atlanta', 'GA'), \n",
    "          ('New York', 'NY'), \n",
    "          ('New Orleans', 'LA'), \n",
    "          ('Seattle', 'WA'), \n",
    "          ('Detroit', 'MI')]\n",
    "\n",
    "# set weeks where weekly data available\n",
    "weeks = ['03-01', \n",
    "         '03-08', \n",
    "         '03-15', \n",
    "         '03-22', \n",
    "         '03-29', \n",
    "         '04-05', \n",
    "         '04-12', \n",
    "         '04-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter and concatenate weekly data\n",
    "df = pd.DataFrame()\n",
    "for week in weeks:\n",
    "    filepath = './v1/main-file/2020-{}-weekly-patterns.csv.gz'.format(week)\n",
    "    if week=='04-05':\n",
    "        filepath = './v1/main-file/2020-04-05-weekly-patterns-corrected.csv.gz'\n",
    "    weekly_df = pd.read_csv(filepath)\n",
    "    for city, state in cities:\n",
    "        subset = weekly_df[(weekly_df.city==city) & (weekly_df.region==state)]\n",
    "        df = pd.concat([df, subset])\n",
    "    del weekly_df\n",
    "\n",
    "# save to pickle\n",
    "df.to_pickle('./data/weekly_cities_03-01_through_04-19.csv.gz', \n",
    "             compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for filtered concatenated weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from pickle\n",
    "df = pd.read_pickle('./data/weekly_cities_03-01_through_04-19.csv.gz', \n",
    "                    compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_list(x):\n",
    "    # converts string of format '[1,2,3]' to list of ints\n",
    "    return np.fromstring(x[1:-1], dtype=int, sep=',')\n",
    "\n",
    "def filter_cities(dataset):\n",
    "    # narrow data to the cities of interest\n",
    "    filtered = pd.DataFrame()\n",
    "    for city, state in cities:\n",
    "        subset = dataset[(dataset.city==city) & (dataset.region==state)]\n",
    "        filtered = pd.concat([filtered, subset])\n",
    "    return filtered\n",
    "\n",
    "def days_and_dates(data, time_type):\n",
    "    \n",
    "    assert time_type=='seconds' or time_type=='YYYY-MM-DD', \\\n",
    "            'time_type must be \\\"seconds\\\" or \\\"YYYY-MM-DD\\\" '\n",
    "\n",
    "    # turn 'visits by day' into list\n",
    "    data['visits_zip'] = data['visits_by_day'].apply(vis_list)\n",
    "    \n",
    "    # create list of all columns except 'visits_zip'\n",
    "    other_cols = list(data.columns)\n",
    "    other_cols.remove('visits_zip')\n",
    "\n",
    "    # Create 1 row for each day\n",
    "    '''\n",
    "    Adapted from: \n",
    "    https://stackoverflow.com/questions/53860398/\n",
    "    pandas-dataframe-how-do-i-split-one-row-into-multiple-rows-by-multi-value-colum\n",
    "    '''\n",
    "    data = data.set_index(other_cols)['visits_zip'] \\\n",
    "               .apply(pd.Series).stack().reset_index() \\\n",
    "               .rename(columns={0:'visits', 'level_{}' \\\n",
    "                                   .format(len(other_cols)): 'days'})\n",
    "    \n",
    "    if time_type=='seconds':\n",
    "        # For historical data, timestamp in UTC in seconds since January 1, 1970\n",
    "        data['start_date'] = pd.to_timedelta(data['date_range_start'], 's') \\\n",
    "                                    + pd.to_datetime('1970, 1, 1')\n",
    "    if time_type=='YYYY-MM-DD':\n",
    "        # For new weekly data, ISO 8601 format of YYYY-MM-DDTHH:mm:SSÂ±hh:mm \n",
    "        # (local time with offset from GMT)\n",
    "        # The start time will be 12 a.m. Sunday in local time.\n",
    "        data['start_date'] = pd.to_datetime(data['date_range_start'] \\\n",
    "                                                   .map(lambda x: x[:10]))\n",
    "\n",
    "    #convert days to time delta\n",
    "    data['days'] = pd.to_timedelta(data['days'], 'd')\n",
    "\n",
    "    # compute date\n",
    "    data['date'] = data['start_date'] + data['days']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process 2020 data\n",
    "df = days_and_dates(df, time_type='YYYY-MM-DD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "df.to_pickle('./data/daily_cities_03-01-2020_04-25-2020.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in historical data for march 2019\n",
    "mar19 = pd.concat([pd.read_csv('./data/Mar19-AllPatterns-PATTERNS-2019_03-2020-03-23/patterns-part1.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Mar19-AllPatterns-PATTERNS-2019_03-2020-03-23/patterns-part2.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Mar19-AllPatterns-PATTERNS-2019_03-2020-03-23/patterns-part3.csv.gz',\n",
    "                   compression='gzip')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process march 2019 data\n",
    "mar19 = filter_cities(mar19)\n",
    "mar19 = days_and_dates(mar19, time_type='seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "mar19.to_pickle('./data/daily_cities_03-01-2019_03-31-2019.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in historical data for april 2019\n",
    "apr19 = pd.concat([pd.read_csv('./data/Apr19-AllPatterns-PATTERNS-2019_04-2020-03-23/patterns-part1.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Apr19-AllPatterns-PATTERNS-2019_04-2020-03-23/patterns-part2.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Apr19-AllPatterns-PATTERNS-2019_04-2020-03-23/patterns-part3.csv.gz',\n",
    "                   compression='gzip')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process april 2019 data\n",
    "apr19 = filter_cities(apr19)\n",
    "apr19 = days_and_dates(apr19, time_type='seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "apr19.to_pickle('./data/daily_cities_04-01-2019_04-30-2019.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for filtered, processed daily data from March-April 2019 and 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from pickle\n",
    "df = pd.read_pickle('./data/daily_cities_03-01-2020_04-25-2020.csv.gz',\n",
    "                       compression='gzip')\n",
    "# read from pickle\n",
    "mar19 = pd.read_pickle('./data/daily_cities_03-01-2019_03-31-2019.csv.gz',\n",
    "                       compression='gzip')\n",
    "# read from pickle\n",
    "apr19 = pd.read_pickle('./data/daily_cities_04-01-2019_04-30-2019.csv.gz',\n",
    "                       compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one frame for 2019 data\n",
    "old=pd.concat([mar19, apr19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create date column without year\n",
    "old['date_yearless'] = old['date'].dt.strftime('%m-%d')\n",
    "df['date_yearless'] = df['date'].dt.strftime('%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join 2019 and 2020 datasets\n",
    "data = df.join(old.set_index(['date_yearless', 'safegraph_place_id']), \n",
    "               on=['date_yearless', 'safegraph_place_id'], how='inner',\n",
    "               lsuffix='_new', rsuffix='_old', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove and rename duplicate columns after join\n",
    "for col in old.columns:\n",
    "    if (col+'_new' in data.columns) and (col+'_old' in data.columns):\n",
    "        if all(data[col+'_new']==data[col+'_old']):\n",
    "            data = data.rename(columns={col+'_new':col}).drop(columns=[col+'_old'])\n",
    "\n",
    "# rename columns with only one source frame\n",
    "for col in data.columns:\n",
    "    if (col in df.columns) and (col not in old.columns):\n",
    "        data = data.rename(columns={col: col+'_new'})\n",
    "    if (col not in df.columns) and (col in old.columns):\n",
    "        data = data.rename(columns={col: col+'_old'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "data.to_pickle('./data/joined_03-01_04-25.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for joined 2019/2020 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/joined_03-01_04-25.csv.gz',\n",
    "                      compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view earliest date\n",
    "# data.date_yearless.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view latest date\n",
    "# data.date_yearless.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all columns\n",
    "# sorted(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #view data overview\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rows where the location name has changed\n",
    "# data.loc[data['location_name_new']!= \\\n",
    "#          data['location_name_old']][['location_name_new', 'location_name_old']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
