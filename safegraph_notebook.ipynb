{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import ast \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "\n",
    "# set cities of interest\n",
    "cities = [('Atlanta', 'GA'), \n",
    "          ('New York', 'NY'), \n",
    "          ('New Orleans', 'LA'), \n",
    "          ('Seattle', 'WA'), \n",
    "          ('Detroit', 'MI')]\n",
    "\n",
    "# set weeks where weekly data available\n",
    "weeks = ['03-01', \n",
    "         '03-08', \n",
    "         '03-15', \n",
    "         '03-22', \n",
    "         '03-29', \n",
    "         '04-05', \n",
    "         '04-12', \n",
    "         '04-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter and concatenate weekly data\n",
    "df = pd.DataFrame()\n",
    "for week in weeks:\n",
    "    filepath = './v1/main-file/2020-{}-weekly-patterns.csv.gz'.format(week)\n",
    "    if week=='04-05':\n",
    "        filepath = './v1/main-file/2020-04-05-weekly-patterns-corrected.csv.gz'\n",
    "    weekly_df = pd.read_csv(filepath)\n",
    "    for city, state in cities:\n",
    "        subset = weekly_df[(weekly_df.city==city) & (weekly_df.region==state)]\n",
    "        df = pd.concat([df, subset])\n",
    "    del weekly_df\n",
    "\n",
    "# save to pickle\n",
    "df.to_pickle('./data/weekly_cities_03-01_through_04-19.csv.gz', \n",
    "             compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for filtered concatenated weekly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from pickle\n",
    "df = pd.read_pickle('./data/weekly_cities_03-01_through_04-19.csv.gz', \n",
    "                    compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_list(x):\n",
    "    # converts string of format '[1,2,3]' to list of ints\n",
    "    return np.fromstring(x[1:-1], dtype=int, sep=',')\n",
    "\n",
    "def filter_cities(dataset):\n",
    "    # narrow data to the cities of interest\n",
    "    filtered = pd.DataFrame()\n",
    "    for city, state in cities:\n",
    "        subset = dataset[(dataset.city==city) & (dataset.region==state)]\n",
    "        filtered = pd.concat([filtered, subset])\n",
    "    return filtered\n",
    "\n",
    "def days_and_dates(data, time_type):\n",
    "    \n",
    "    assert time_type=='seconds' or time_type=='YYYY-MM-DD', \\\n",
    "            'time_type must be \\\"seconds\\\" or \\\"YYYY-MM-DD\\\" '\n",
    "\n",
    "    # turn 'visits by day' into list\n",
    "    data['visits_zip'] = data['visits_by_day'].apply(vis_list)\n",
    "    \n",
    "    # create list of all columns except 'visits_zip'\n",
    "    other_cols = list(data.columns)\n",
    "    other_cols.remove('visits_zip')\n",
    "\n",
    "    # Create 1 row for each day\n",
    "    '''\n",
    "    Adapted from: \n",
    "    https://stackoverflow.com/questions/53860398/\n",
    "    pandas-dataframe-how-do-i-split-one-row-into-multiple-rows-by-multi-value-colum\n",
    "    '''\n",
    "    data = data.set_index(other_cols)['visits_zip'] \\\n",
    "               .apply(pd.Series).stack().reset_index() \\\n",
    "               .rename(columns={0:'visits', 'level_{}' \\\n",
    "                                   .format(len(other_cols)): 'days'})\n",
    "    \n",
    "    if time_type=='seconds':\n",
    "        # For historical data, timestamp in UTC in seconds since January 1, 1970\n",
    "        data['start_date'] = pd.to_timedelta(data['date_range_start'], 's') \\\n",
    "                                    + pd.to_datetime('1970, 1, 1')\n",
    "    if time_type=='YYYY-MM-DD':\n",
    "        # For new weekly data, ISO 8601 format of YYYY-MM-DDTHH:mm:SSÂ±hh:mm \n",
    "        # (local time with offset from GMT)\n",
    "        # The start time will be 12 a.m. Sunday in local time.\n",
    "        data['start_date'] = pd.to_datetime(data['date_range_start'] \\\n",
    "                                                   .map(lambda x: x[:10]))\n",
    "\n",
    "    #convert days to time delta\n",
    "    data['days'] = pd.to_timedelta(data['days'], 'd')\n",
    "\n",
    "    # compute date\n",
    "    data['date'] = data['start_date'] + data['days']\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process mar-april 2020 data\n",
    "df = days_and_dates(df, time_type='YYYY-MM-DD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "df.to_pickle('./data/daily_cities_03-01-2020_04-25-2020.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in feb 2020 data\n",
    "feb20 = pd.concat([pd.read_csv('./data/Feb20-AllPatterns-PATTERNS-2020_02-2020-03-23/patterns-part1.csv.gz', \n",
    "                               compression='gzip'), \n",
    "                   pd.read_csv('./data/Feb20-AllPatterns-PATTERNS-2020_02-2020-03-23/patterns-part2.csv.gz', \n",
    "                               compression='gzip'), \n",
    "                   pd.read_csv('./data/Feb20-AllPatterns-PATTERNS-2020_02-2020-03-23/patterns-part3.csv.gz', \n",
    "                               compression='gzip')])\n",
    "\n",
    "# process feb 2020 data\n",
    "feb20 = filter_cities(feb20)\n",
    "feb20 = days_and_dates(feb20, time_type='seconds')\n",
    "\n",
    "# save to pickle\n",
    "feb20.to_pickle('./data/daily_cities_02-01-2020_02-29-2020.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in jan 2020 data\n",
    "jan20 = pd.concat([pd.read_csv('./data/Jan20-AllPatterns-PATTERNS-2020_01-2020-03-23/patterns-part1.csv.gz', \n",
    "                               compression='gzip'), \n",
    "                   pd.read_csv('./data/Jan20-AllPatterns-PATTERNS-2020_01-2020-03-23/patterns-part2.csv.gz', \n",
    "                               compression='gzip'), \n",
    "                   pd.read_csv('./data/Jan20-AllPatterns-PATTERNS-2020_01-2020-03-23/patterns-part3.csv.gz', \n",
    "                               compression='gzip')])\n",
    "\n",
    "# process jan 2020 data\n",
    "jan20 = filter_cities(jan20)\n",
    "jan20 = days_and_dates(jan20, time_type='seconds')\n",
    "\n",
    "# save to pickle\n",
    "jan20.to_pickle('./data/daily_cities_01-01-2020_01-31-2020.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in historical data for jan 2019\n",
    "jan19 = pd.concat([pd.read_csv('./data/Jan19-AllRecords-PATTERNS-2019_01-2020-03-26/patterns-part1.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Jan19-AllRecords-PATTERNS-2019_01-2020-03-26/patterns-part2.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Jan19-AllRecords-PATTERNS-2019_01-2020-03-26/patterns-part3.csv.gz',\n",
    "                   compression='gzip')])\n",
    "\n",
    "# process jan 2019 data\n",
    "jan19 = filter_cities(jan19)\n",
    "jan19 = days_and_dates(jan19, time_type='seconds')\n",
    "\n",
    "# save to pickle\n",
    "jan19.to_pickle('./data/daily_cities_01-01-2019_01-31-2019.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in historical data for feb 2019\n",
    "feb19 = pd.concat([pd.read_csv('./data/Feb19-AllRecords-PATTERNS-2019_02-2020-03-26/patterns-part1.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Feb19-AllRecords-PATTERNS-2019_02-2020-03-26/patterns-part2.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Feb19-AllRecords-PATTERNS-2019_02-2020-03-26/patterns-part3.csv.gz',\n",
    "                   compression='gzip')])\n",
    "\n",
    "# process feb 2019 data\n",
    "feb19 = filter_cities(feb19)\n",
    "feb19 = days_and_dates(feb19, time_type='seconds')\n",
    "\n",
    "# save to pickle\n",
    "feb19.to_pickle('./data/daily_cities_02-01-2019_02-28-2019.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in historical data for march 2019\n",
    "mar19 = pd.concat([pd.read_csv('./data/Mar19-AllPatterns-PATTERNS-2019_03-2020-03-23/patterns-part1.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Mar19-AllPatterns-PATTERNS-2019_03-2020-03-23/patterns-part2.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Mar19-AllPatterns-PATTERNS-2019_03-2020-03-23/patterns-part3.csv.gz',\n",
    "                   compression='gzip')])\n",
    "\n",
    "# process march 2019 data\n",
    "mar19 = filter_cities(mar19)\n",
    "mar19 = days_and_dates(mar19, time_type='seconds')\n",
    "\n",
    "# save to pickle\n",
    "mar19.to_pickle('./data/daily_cities_03-01-2019_03-31-2019.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in historical data for april 2019\n",
    "apr19 = pd.concat([pd.read_csv('./data/Apr19-AllPatterns-PATTERNS-2019_04-2020-03-23/patterns-part1.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Apr19-AllPatterns-PATTERNS-2019_04-2020-03-23/patterns-part2.csv.gz',\n",
    "                   compression='gzip'), \n",
    "                   pd.read_csv('./data/Apr19-AllPatterns-PATTERNS-2019_04-2020-03-23/patterns-part3.csv.gz',\n",
    "                   compression='gzip')])\n",
    "\n",
    "# process april 2019 data\n",
    "apr19 = filter_cities(apr19)\n",
    "apr19 = days_and_dates(apr19, time_type='seconds')\n",
    "\n",
    "# save to pickle\n",
    "apr19.to_pickle('./data/daily_cities_04-01-2019_04-30-2019.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for filtered, processed daily data from Jan-April 2019 and 2020 \n",
    "### [OUTDATED PROCESS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from pickles\n",
    "\n",
    "jan20 = pd.read_pickle('./data/daily_cities_01-01-2020_01-31-2020.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "feb20 = pd.read_pickle('./data/daily_cities_02-01-2020_02-29-2020.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "marapr20 = pd.read_pickle('./data/daily_cities_03-01-2020_04-25-2020.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "jan19 = pd.read_pickle('./data/daily_cities_01-01-2019_01-31-2019.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "feb19 = pd.read_pickle('./data/daily_cities_02-01-2019_02-28-2019.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "mar19 = pd.read_pickle('./data/daily_cities_03-01-2019_03-31-2019.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "apr19 = pd.read_pickle('./data/daily_cities_04-01-2019_04-30-2019.csv.gz',\n",
    "                       compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create Mar-Apr dataset, 19/20 joined on day\n",
    "OUTDATED\n",
    "'''\n",
    "\n",
    "# create one frame for 2019 data\n",
    "old=pd.concat([mar19, apr19])\n",
    "\n",
    "# create date column without year\n",
    "old['date_yearless'] = old['date'].dt.strftime('%m-%d')\n",
    "df['date_yearless'] = df['date'].dt.strftime('%m-%d')\n",
    "\n",
    "# join 2019 and 2020 datasets\n",
    "data = df.join(old.set_index(['date_yearless', 'safegraph_place_id']), \n",
    "               on=['date_yearless', 'safegraph_place_id'], how='inner',\n",
    "               lsuffix='_new', rsuffix='_old', sort=True)\n",
    "\n",
    "# remove and rename duplicate columns after join\n",
    "for col in old.columns:\n",
    "    if (col+'_new' in data.columns) and (col+'_old' in data.columns):\n",
    "        if all(data[col+'_new']==data[col+'_old']):\n",
    "            data = data.rename(columns={col+'_new':col}).drop(columns=[col+'_old'])\n",
    "\n",
    "# rename columns with only one source frame\n",
    "for col in data.columns:\n",
    "    if (col in df.columns) and (col not in old.columns):\n",
    "        data = data.rename(columns={col: col+'_new'})\n",
    "    if (col not in df.columns) and (col in old.columns):\n",
    "        data = data.rename(columns={col: col+'_old'})\n",
    "        \n",
    "# save to pickle\n",
    "data.to_pickle('./data/joined_03-01_04-25.csv.gz',\n",
    "                compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for joined Mar-20 2019/2020 dataset, joined on day \n",
    "### [OUTDATED]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/joined_03-01_04-25.csv.gz',\n",
    "                      compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view earliest date\n",
    "# data.date_yearless.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view latest date\n",
    "# data.date_yearless.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view all columns\n",
    "# sorted(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #view data overview\n",
    "# data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view rows where the location name has changed\n",
    "# data.loc[data['location_name_new']!= \\\n",
    "#          data['location_name_old']][['location_name_new', 'location_name_old']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for filtered, processed daily data from March-April 2019 and 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mar-april 2019 and 2020 dataframes, from pickles\n",
    "\n",
    "df20 = pd.read_pickle('./data/daily_cities_03-01-2020_04-25-2020.csv.gz',\n",
    "                       compression='gzip')\n",
    "\n",
    "df19 = pd.concat([pd.read_pickle('./data/daily_cities_03-01-2019_03-31-2019.csv.gz',\n",
    "                       compression='gzip'), \n",
    "                  pd.read_pickle('./data/daily_cities_04-01-2019_04-30-2019.csv.gz',\n",
    "                       compression='gzip')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create week of year column, starting sunday, for groupby\n",
    "df20['week'] = df20['date'].apply(lambda x: (x + dt.timedelta(days=1)).week)\n",
    "df19['week'] = df19['date'].apply(lambda x: (x + dt.timedelta(days=1)).week)\n",
    "\n",
    "# create max_hourly_visits (for new data only)\n",
    "df20['max_hourly_visits'] = df20['visits_by_each_hour']\\\n",
    "                                            .apply(lambda x: max(ast.literal_eval(x)))\n",
    "\n",
    "#create num_related_same_day_brand\n",
    "df20['num_related_same_day_brand'] = df20['related_same_day_brand']\\\n",
    "                                            .apply(lambda x: len(ast.literal_eval(x)))\n",
    "df19['num_related_same_day_brand'] = df19['related_same_day_brand']\\\n",
    "                                            .apply(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "# replace na with string of empty dictionary\n",
    "# for visit-related dicts with missing values \n",
    "df20['visitor_country_of_origin'].fillna('{}', inplace=True)\n",
    "df19['visitor_country_of_origin'].fillna('{}', inplace=True)\n",
    "df20['visitor_home_cbgs'].fillna('{}', inplace=True)\n",
    "df19['visitor_home_cbgs'].fillna('{}', inplace=True)\n",
    "\n",
    "#create visits_per_visitor_country_of_origin\n",
    "df20['num_visitor_country_of_origin'] = df20['visitor_country_of_origin']\\\n",
    "                                                .apply(lambda x: len(ast.literal_eval(x)))\n",
    "df19['num_visitor_country_of_origin'] = df19['visitor_country_of_origin']\\\n",
    "                                              .apply(lambda x: len(ast.literal_eval(x)))\n",
    "\n",
    "#create visits_per_visitor_home_cbgs\n",
    "df20['num_visitor_home_cbgs'] = df20['visitor_home_cbgs']\\\n",
    "                                              .apply(lambda x: len(ast.literal_eval(x)))\n",
    "df19['num_visitor_home_cbgs'] = df19['visitor_home_cbgs']\\\n",
    "                                              .apply(lambda x: len(ast.literal_eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_dict_20 = {'visits':'sum', # sum the column of interest\n",
    "               'raw_visit_counts': np.nanmedian,# to check sum is correct\n",
    "                # take \"first\" for location-based features (assuming doesnt change)\n",
    "                'location_name':'first', # to understand records when inspecting manually\n",
    "                'city': 'first',\n",
    "                'region':'first',\n",
    "                'postal_code': 'first',\n",
    "                'naics_code':'first',\n",
    "                'brands': 'first',\n",
    "                # take \"median\" for numerical visit-based features\n",
    "                'distance_from_home': np.nanmedian,\n",
    "                'raw_visitor_counts': np.nanmedian,\n",
    "                'median_dwell': np.nanmedian,\n",
    "                'num_visitor_country_of_origin': np.nanmedian,\n",
    "                'num_visitor_home_cbgs': np.nanmedian,\n",
    "                'num_related_same_day_brand': np.nanmedian,\n",
    "                'max_hourly_visits': np.nanmedian,\n",
    "                'related_same_day_brand':'first' #incase need to use more\n",
    "                  }\n",
    "\n",
    "agg_dict_19 = {'visits':'sum', # sum the column of interest\n",
    "                # take \"median\" for numerical visit-based features\n",
    "                'distance_from_home': np.nanmedian,\n",
    "                'median_dwell': np.nanmedian,\n",
    "                'num_related_same_day_brand': np.nanmedian,\n",
    "                'related_same_day_brand':'first' #incase need to use more\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grouby week and location, and aggregate\n",
    "weekly19 = df19.groupby(['week', 'safegraph_place_id']).agg(agg_dict_19)\n",
    "weekly20 = df20.groupby(['week', 'safegraph_place_id']).agg(agg_dict_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join 2019 and 2020 \n",
    "joined = weekly20.join(weekly19, on=['week', 'safegraph_place_id'], how='inner',\n",
    "                       lsuffix='_2020', rsuffix='_2019', sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joined[joined['raw_visit_counts']!=joined['visits_2020']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 rows in which raw_visit_counts does not equal the sum of the daily visits. In each of these 20 cases, raw_visit_counts is exactly 1 higher than the sum. We will keep the sum, for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3109\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(joined[joined['visits_2019']==0]))\n",
    "print(len(joined[joined['visits_2020']==0]))\n",
    "print(len(joined[joined['raw_visit_counts']==0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are there so many more 0 visits in 2019? Has the 2020 data been altered? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "Benford's Law Plots\n",
    "'''\n",
    "benford_19 = list(joined['visits_2019'].apply(lambda x: str(x)[0]))\n",
    "benford_20 = list(joined['visits_2020'].apply(lambda x: str(x)[0]))\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "ax[0].hist(sorted(benford_19))\n",
    "ax[0].set_title('Benford\\'s Law: 2019')\n",
    "ax[1].hist(sorted(benford_20))\n",
    "ax[1].set_title('Benford\\'s Law: 2020')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 1 to all visit counts, to avoid divide-by-zero errors\n",
    "joined['visits_2019'] = joined['visits_2019'] + 1\n",
    "joined['visits_2020'] = joined['visits_2020'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate % change in visits from 2019 to 2020\n",
    "joined['change_in_visits'] = (joined['visits_2020'] - joined['visits_2019'])\\\n",
    "                                                /joined['visits_2019']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    388440.000000\n",
       "mean         -0.104214\n",
       "std           5.837428\n",
       "min          -0.999824\n",
       "25%          -0.821053\n",
       "50%          -0.594771\n",
       "75%          -0.200000\n",
       "max        1605.000000\n",
       "Name: change_in_visits, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe our new target variable\n",
    "joined['change_in_visits'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many places have more than 100x the visits they did in 2019?\n",
    "len(joined[joined['change_in_visits']>100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "joined.to_pickle('./data/joined_weekly_03-01_04-25.csv.gz', \n",
    "             compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint - load from here for joined weekly data, March-April 2019/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./data/joined_weekly_03-01_04-25.csv.gz', \n",
    "             compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1-week lag columns\n",
    "lag_cols = ['visits_2020', 'distance_from_home_2020', 'raw_visitor_counts', \n",
    "            'median_dwell_2020', 'num_visitor_country_of_origin',\n",
    "            'num_visitor_home_cbgs', 'num_related_same_day_brand_2020',\n",
    "            'max_hourly_visits', 'related_same_day_brand_2020', 'visits_2019',\n",
    "            'distance_from_home_2019', 'median_dwell_2019',\n",
    "            'num_related_same_day_brand_2019', 'related_same_day_brand_2019',\n",
    "            'change_in_visits']\n",
    "\n",
    "for col in lag_cols:\n",
    "    df[col+'_lastweek'] = df.groupby('safegraph_place_id')[col].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reverse-lag columns for 2019 features\n",
    "nextweek_cols = ['visits_2019', 'distance_from_home_2019', 'median_dwell_2019',\n",
    "                   'num_related_same_day_brand_2019', 'related_same_day_brand_2019']\n",
    "\n",
    "for col in nextweek_cols:\n",
    "    df[col+'_nextweek'] = df.groupby('safegraph_place_id')[col].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target column (next week's change in visits)\n",
    "df['target'] = df.groupby('safegraph_place_id')['change_in_visits'].shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[df.safegraph_place_id=='sg:00001d1ef23c4747bfefd65b6fd0e514'][['week',\n",
    "#                                                                   'change_in_visits_lastweek', \n",
    "#                                                                   'change_in_visits', \n",
    "#                                                                   'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.safegraph_place_id=='sg:00001d1ef23c4747bfefd65b6fd0e514'][['week',\n",
    "#                                                                   'visits_2019_lastweek', \n",
    "#                                                                   'visits_2019', \n",
    "#                                                                   'visits_2019_nextweek', \n",
    "#                                                                   'visits_2020_lastweek', \n",
    "#                                                                   'visits_2020']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the lag-columns reduces our dataset from 7 weeks to 6 weeks. Is this worth it? We could test the theory if we have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safegraph_place_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>52230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>51999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>47816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>47182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>47076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      safegraph_place_id\n",
       "week                    \n",
       "10                 52230\n",
       "11                 51999\n",
       "12                 50210\n",
       "13                 47816\n",
       "14                 47182\n",
       "15                 46099\n",
       "16                 45828\n",
       "17                 47076"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many locations are in each week?\n",
    "df.groupby('week').agg({'safegraph_place_id':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to remove IDs not present in all weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of ids in each week\n",
    "id_lists = df.groupby('week').agg({'safegraph_place_id':list})\\\n",
    "                                    ['safegraph_place_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get intersection of all weeks\n",
    "final_set = set(id_lists[0])\n",
    "for i in range(1,len(id_lists)):\n",
    "    final_set = final_set & set(id_lists[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the IDs that appear in all groups\n",
    "df = df[df.safegraph_place_id.isin(final_set)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "305416"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New size of DF\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>safegraph_place_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>38177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      safegraph_place_id\n",
       "week                    \n",
       "10                 38177\n",
       "11                 38177\n",
       "12                 38177\n",
       "13                 38177\n",
       "14                 38177\n",
       "15                 38177\n",
       "16                 38177\n",
       "17                 38177"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that this worked\n",
    "df.groupby('week').agg({'safegraph_place_id':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing NAICS code:  336\n",
      "Number of distinct locations missing NAICS code:  42\n"
     ]
    }
   ],
   "source": [
    "# Check for missing NAICS codes\n",
    "print('Number of rows missing NAICS code: ', len(df[df.naics_code.isna()]))\n",
    "print('Number of distinct locations missing NAICS code: ', \n",
    "      len(df[df.naics_code.isna()].groupby('safegraph_place_id')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NAICS code as Nan:  0\n",
      "Number of rows with NAICS code as '0':  336\n",
      "Number of distinct locations missing NAICS code:  42\n"
     ]
    }
   ],
   "source": [
    "# create NAICS lookup\n",
    "naics_lists = df.groupby('safegraph_place_id').agg({'naics_code':list})\n",
    "\n",
    "# Replace NaN NAICS values\n",
    "df.loc[df.naics_code.isna(), 'naics_code'] = df.safegraph_place_id.apply(lambda x: \\\n",
    "              str(int(mode(list(naics_lists.loc[x])[0], nan_policy='omit')[0][0]))) \n",
    "                                                                        \n",
    "print('Number of rows with NAICS code as Nan: ', len(df[df.naics_code.isna()]))\n",
    "print('Number of rows with NAICS code as \\'0\\': ', len(df[df.naics_code=='0']))\n",
    "print('Number of distinct locations missing NAICS code: ', \n",
    "      len(df[df.naics_code=='0'].groupby('safegraph_place_id')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This didn't help at all. We could try to do a second pass with brand? Or use discarded rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features for NAICS substrings\n",
    "\n",
    "def naics_parts(code, i):\n",
    "    if code=='0':\n",
    "        return np.nan\n",
    "    code=str(code)\n",
    "    return code[:i]\n",
    "\n",
    "for i in range(2,6):\n",
    "    df['naics_{}'.format(i)] = df['naics_code'].apply(naics_parts, args=(i,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "df.to_pickle('./data/joined_weekly_03-01_04-25_feat_filt.csv.gz', \n",
    "             compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>week</th>\n",
       "      <th>U_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   week U_week\n",
       "0    10     09\n",
       "1    10     09\n",
       "2    10     09\n",
       "3    10     09\n",
       "4    10     09"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test U_week vs current method\n",
    "dfcopy = df20.copy()\n",
    "dfcopy['U_week'] = dfcopy['date'].dt.strftime('%U')\n",
    "\n",
    "dfcopy[['week', 'U_week']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensure is always 1 more than current method\n",
    "all(dfcopy['week']==dfcopy['U_week'].astype(int)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decrease week by 1 to align with strftime '%U'\n",
    "df['week'] = df['week']-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop raw_visit_counts (was only used for quality checking)\n",
    "df.drop(columns=['raw_visit_counts'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of columns to drop and to use in joining\n",
    "to_drop=['location_name', 'city', 'region', 'brands']\n",
    "joining_columns=['week', 'postal_code']\n",
    "model_df = df.drop(columns=to_drop+joining_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance_from_home_2020\n",
      "80225\n",
      "\n",
      "\n",
      "distance_from_home_2019\n",
      "252\n",
      "\n",
      "\n",
      "target\n",
      "38177\n",
      "\n",
      "\n",
      "naics_2\n",
      "336\n",
      "\n",
      "\n",
      "naics_3\n",
      "336\n",
      "\n",
      "\n",
      "naics_4\n",
      "336\n",
      "\n",
      "\n",
      "naics_5\n",
      "336\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view number of remaining na values\n",
    "for col in model_df.columns:\n",
    "    if model_df[col].isna().sum()>0:\n",
    "        if '_lastweek' not in col:\n",
    "            if '_nextweek' not in col:\n",
    "                print(col)\n",
    "                print(model_df[col].isna().sum())\n",
    "                print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop 2020 distance because missing too often\n",
    "to_drop.append('distance_from_home_2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing 2019 distance with training median\n",
    "# Assume train/val split of 5 weeks vs 1 week\n",
    "latest_train_week=15\n",
    "\n",
    "# create distance missing feature\n",
    "df['distance_missing']=0\n",
    "df.loc[df.distance_from_home_2019.isna(), 'distance_missing'] = 1\n",
    "\n",
    "# Replace NaN with median from training weeks\n",
    "df.loc[df.distance_from_home_2019.isna(), 'distance_from_home_2019'] = \\\n",
    "        df.loc[df.week<=latest_train_week, 'distance_from_home_2019'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing NAICS with 0\n",
    "for i in range(2,6):\n",
    "    df['naics_{}'.format(i)].fillna('0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop related same day brand dictionaries\n",
    "brands = [x for x in df.columns if x.startswith('related_same_day_brand')]\n",
    "to_drop = to_drop + brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop place ID\n",
    "to_drop.append('safegraph_place_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change_in_visits',\n",
       " 'change_in_visits_lastweek',\n",
       " 'distance_from_home_2019',\n",
       " 'distance_from_home_2019_lastweek',\n",
       " 'distance_from_home_2019_nextweek',\n",
       " 'distance_from_home_2020_lastweek',\n",
       " 'distance_missing',\n",
       " 'max_hourly_visits',\n",
       " 'max_hourly_visits_lastweek',\n",
       " 'median_dwell_2019',\n",
       " 'median_dwell_2019_lastweek',\n",
       " 'median_dwell_2019_nextweek',\n",
       " 'median_dwell_2020',\n",
       " 'median_dwell_2020_lastweek',\n",
       " 'naics_2',\n",
       " 'naics_3',\n",
       " 'naics_4',\n",
       " 'naics_5',\n",
       " 'naics_code',\n",
       " 'num_related_same_day_brand_2019',\n",
       " 'num_related_same_day_brand_2019_lastweek',\n",
       " 'num_related_same_day_brand_2019_nextweek',\n",
       " 'num_related_same_day_brand_2020',\n",
       " 'num_related_same_day_brand_2020_lastweek',\n",
       " 'num_visitor_country_of_origin',\n",
       " 'num_visitor_country_of_origin_lastweek',\n",
       " 'num_visitor_home_cbgs',\n",
       " 'num_visitor_home_cbgs_lastweek',\n",
       " 'raw_visitor_counts',\n",
       " 'raw_visitor_counts_lastweek',\n",
       " 'target',\n",
       " 'visits_2019',\n",
       " 'visits_2019_lastweek',\n",
       " 'visits_2019_nextweek',\n",
       " 'visits_2020',\n",
       " 'visits_2020_lastweek']"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view modeling columns\n",
    "model_df = df.drop(columns=to_drop+joining_columns)\n",
    "sorted(model_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing decimals from naics_code\n",
    "df['naics_code']=df['naics_code'].astype(int).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 1st and last weeks\n",
    "df = df[(df.week!=df.week.min()) & (df.week!=df.week.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create non-OHE version to share\n",
    "share_df = df.drop(columns=to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create OHE version to share\n",
    "naics_ohe = pd.get_dummies(share_df, columns=[x for x in share_df.columns if x.startswith('naics_')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop final dummy from each level (choose to drop 0s)\n",
    "naics_0s = ['naics_{}_0'.format(x) for x in range(2,6)]\n",
    "naics_0s.append('naics_code_0')\n",
    "naics_ohe.drop(columns=naics_0s, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "naics_ohe.to_csv('./data/safegraph_ohe.csv.gz', \n",
    "                 compression='gzip', index=False)\n",
    "\n",
    "share_df.to_csv('./data/safegraph.csv.gz', \n",
    "                 compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "naics_ohe = pd.read_csv('./data/safegraph_ohe.csv.gz', \n",
    "                 compression='gzip')\n",
    "\n",
    "share_df = pd.read_csv('./data/safegraph.csv.gz', \n",
    "                 compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naics_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "# share_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
